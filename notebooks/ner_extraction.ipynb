{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pre-trained NER model from Hugging Face, specifically dbmdz/bert-large-cased-finetuned-conll03-english which is a pre-trained BERT model fine-tuned specifically for Named Entity Recognition (NER) on the CoNLL-03 dataset. It recognizes entities like PER (Person), ORG (Organization), LOC (Location), and MISC (Miscellaneous).\n",
    "\n",
    "https://www.aimodels.fyi/models/huggingFace/bert-large-cased-finetuned-conll03-english-dbmdz\n",
    "\n",
    "We’ll apply the model to our preprocessed text to extract entities such as persons, organizations, locations, and other entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner_model = pipeline(\"ner\", \n",
    "#                     model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", \n",
    "#                     aggregation_strategy=\"simple\")\n",
    "\n",
    "ner_model = pipeline(\"ner\", \n",
    "                     model=\"elastic/distilbert-base-cased-finetuned-conll03-english\",  # DistilBERT pre-trained on CoNLL-03 NER\n",
    "                     aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is expected behavior for the pre-trained NER model. \n",
    "The warning simply means that the weights related to tasks other than NER (like sequence classification) are not being used, which is correct for our purpose of NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obama = pd.read_json('../data/preprocessed/obama_preprocessed.json', lines=True)\n",
    "df_trump = pd.read_json('../data/preprocessed/trump_preprocessed.json', lines=True)\n",
    "df_biden = pd.read_json('../data/preprocessed/biden_preprocessed.json', lines=True)\n",
    "\n",
    "df_obama = df_obama[['publish_date', 'processed_text']].sample(n=7000, random_state=42)\n",
    "df_trump = df_trump[['publish_date', 'processed_text']].sample(n=7000, random_state=42)\n",
    "df_biden = df_biden[['publish_date', 'processed_text']].sample(n=7000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(texts):\n",
    "    entities_list = []\n",
    "    for text in tqdm(texts, desc=\"Processing NER\", ncols=100):\n",
    "        entities = ner_model(text)\n",
    "        entities_list.append([(ent['word'], ent['entity_group']) for ent in entities])\n",
    "    return entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NER: 100%|███████████████████████████████████████████| 7000/7000 [16:40<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER applied and entities saved for Obama dataset.\n"
     ]
    }
   ],
   "source": [
    "df_obama['entities'] = extract_entities(df_obama['processed_text'])\n",
    "df_obama = df_obama.drop(columns=['processed_text'])\n",
    "\n",
    "df_obama.to_json('../data/entities/DistilBERTobama_entities.json', orient='records', lines=True)\n",
    "print(\"NER applied and entities saved for Obama dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NER: 100%|███████████████████████████████████████████| 7000/7000 [14:55<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER applied and entities saved for Trump dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NER: 100%|███████████████████████████████████████████| 7000/7000 [14:03<00:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER applied and entities saved for Biden dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Trump dataset\n",
    "df_trump['entities'] = extract_entities(df_trump['processed_text'])\n",
    "df_trump = df_trump.drop(columns=['processed_text'])\n",
    "\n",
    "df_trump.to_json('../data/entities/DistilBERTtrump_entities.json', orient='records', lines=True)\n",
    "print(\"NER applied and entities saved for Trump dataset.\")\n",
    "\n",
    "\n",
    "# Biden dataset\n",
    "df_biden['entities'] = extract_entities(df_biden['processed_text'])\n",
    "df_biden = df_biden.drop(columns=['processed_text'])\n",
    "\n",
    "df_biden.to_json('../data/entities/DistilBERTbiden_entities.json', orient='records', lines=True)\n",
    "print(\"NER applied and entities saved for Biden dataset.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
