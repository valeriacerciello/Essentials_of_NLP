{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis tokenizing phrases rather than individual words could be a better choice due to the presence of important phrases like \"United States of America\" etc... which might lose context if broken into individual words.\n",
    "\n",
    "We process the text in batches to speed up the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/tokenized', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_phrases(dataset, batch_size=1000):\n",
    "    tokenized_phrases = []\n",
    "    \n",
    "    for doc in nlp.pipe([ \" \".join(entry['text']) if isinstance(entry['text'], list) else entry['text'] for entry in dataset], batch_size=batch_size):\n",
    "        if doc:\n",
    "            phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "            tokenized_phrases.append(phrases)\n",
    "        else:\n",
    "            tokenized_phrases.append([])\n",
    "\n",
    "    return tokenized_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to not lowercase text because our task is NER and some words could lose their distinguishing features when lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned/cleaned_biden_data.json', 'r') as f:\n",
    "    biden_data = json.load(f)\n",
    "\n",
    "with open('../data/cleaned/cleaned_obama_data.json', 'r') as f:\n",
    "    obama_data = json.load(f)\n",
    "\n",
    "with open('../data/cleaned/cleaned_trump_data.json', 'r') as f:\n",
    "    trump_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Biden dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Biden dataset...\")\n",
    "biden_preprocessed_phrases = preprocess_data_phrases(biden_data)\n",
    "with open('../data/tokenized/preprocessed_biden_phrases.json', 'w') as f:\n",
    "    json.dump(biden_preprocessed_phrases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Obama dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Obama dataset...\")\n",
    "obama_preprocessed_phrases = preprocess_data_phrases(obama_data)\n",
    "with open('../data/tokenized/preprocessed_obama_phrases.json', 'w') as f:\n",
    "    json.dump(obama_preprocessed_phrases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Trump dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Trump dataset...\")\n",
    "trump_preprocessed_phrases = preprocess_data_phrases(trump_data)\n",
    "with open('../data/tokenized/preprocessed_trump_phrases.json', 'w') as f:\n",
    "    json.dump(trump_preprocessed_phrases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 14299.131327867508 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Preprocessing completed in {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
