{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'tagger'])  # enable parser for noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/tokenized', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis tokenizing phrases rather than individual words could be a better choice due to the presence of important phrases like \"United States of America\" etc... which might lose context if broken into individual words.\n",
    "\n",
    "We process the text in batches to speed up the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_with_pipe(dataset, batch_size=1000):\n",
    "    tokenized_phrases = []\n",
    "    for doc in nlp.pipe([\" \".join(entry['text']) if isinstance(entry['text'], list) else entry['text'] for entry in dataset], batch_size=batch_size):\n",
    "        phrases = [chunk.text for chunk in doc.noun_chunks]  # Extract noun phrases\n",
    "        tokenized_phrases.append(phrases)\n",
    "    return tokenized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(phrase_list):\n",
    "    return [phrase.lower() for phrase in phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, batch_size=1000):\n",
    "    tokenized_phrases = preprocess_data_with_pipe(dataset, batch_size)\n",
    "    lowercased_phrases = [lowercase_text(phrases) for phrases in tokenized_phrases]\n",
    "    return lowercased_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleaned/cleaned_biden_data.json', 'r') as f:\n",
    "    biden_data = json.load(f)\n",
    "\n",
    "with open('../data/cleaned/cleaned_obama_data.json', 'r') as f:\n",
    "    obama_data = json.load(f)\n",
    "\n",
    "with open('../data/cleaned/cleaned_trump_data.json', 'r') as f:\n",
    "    trump_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing Biden dataset...\")\n",
    "biden_preprocessed_phrases = preprocess_data(biden_data, batch_size=1000)\n",
    "with open('../data/tokenized/preprocessed_biden_phrases.json', 'w') as f:\n",
    "    json.dump(biden_preprocessed_phrases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing Obama dataset...\")\n",
    "obama_preprocessed_phrases = preprocess_data(obama_data, batch_size=1000)\n",
    "with open('../data/tokenized/preprocessed_obama_phrases.json', 'w') as f:\n",
    "    json.dump(obama_preprocessed_phrases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing Trump dataset...\")\n",
    "trump_preprocessed_phrases = preprocess_data(trump_data, batch_size=1000)\n",
    "with open('../data/tokenized/preprocessed_trump_phrases.json', 'w') as f:\n",
    "    json.dump(trump_preprocessed_phrases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Preprocessing competed in {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
