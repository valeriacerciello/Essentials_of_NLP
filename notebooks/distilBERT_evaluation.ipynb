{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9913d623557c3d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:02:06.366824Z",
     "start_time": "2024-10-21T13:02:02.271783Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f79d18e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# In order to compare the 2 models we need to perform the NER of the same speeches in the distilBERT model\n",
    "\n",
    "ner_model = pipeline(\"ner\", \n",
    "                     model=\"elastic/distilbert-base-cased-finetuned-conll03-english\",  # DistilBERT pre-trained on CoNLL-03 NER\n",
    "                     aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f32b322",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to preprocess and merge entities\n",
    "def preprocess_entities(entity_list):\n",
    "    merged_entities = []\n",
    "    temp_entity = []\n",
    "    for entity, label in entity_list:\n",
    "        # Check if the entity is a continuation of the previous one (look for subword markers or periods)\n",
    "        if re.match(r'^[##\\.]', entity) or (temp_entity and re.match(r'[\\.]', temp_entity[-1][0])):\n",
    "            temp_entity.append((entity, label))\n",
    "        else:\n",
    "            if temp_entity:\n",
    "                merged_entities.append((\"\".join(e[0].replace('##', '').replace('.', '') for e in temp_entity), temp_entity[0][1]))\n",
    "            temp_entity = [(entity, label)]\n",
    "    \n",
    "    # Append the last accumulated entity\n",
    "    if temp_entity:\n",
    "        merged_entities.append((\"\".join(e[0].replace('##', '').replace('.', '') for e in temp_entity), temp_entity[0][1]))\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "\n",
    "def extract_entities(texts):\n",
    "    entities_list = []\n",
    "    for text in tqdm(texts, desc=\"Processing NER\", ncols=100):\n",
    "        entities = ner_model(text)\n",
    "        entities_list.append([(ent['word'], ent['entity_group']) for ent in entities])\n",
    "    \n",
    "    merged_entities = [preprocess_entities(entities) for entities in entities_list]\n",
    "    json_style_entities = merged_entities.apply(lambda x: json.dumps([{\"text\": ent[0], \"label\": ent[1]} for ent in x]))\n",
    "    return json_style_entities\n",
    "\n",
    "\n",
    "# Process and save entities\n",
    "def format_and_save(df, filename):\n",
    "    df['entities'] = df['entities'].apply(preprocess_entities)\n",
    "    df['entities'] = df['entities'].apply(lambda x: json.dumps([{\"text\": ent[0], \"label\": ent[1]} for ent in x]))\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_obama = pd.read_csv('../data/preprocessed/sampled_cleaned_obama.csv')\n",
    "df_trump = pd.read_csv('../data/preprocessed/sampled_cleaned_trump.csv')\n",
    "df_biden = pd.read_csv('../data/preprocessed/sampled_cleaned_biden.csv')\n",
    "\n",
    "\n",
    "# Extract entities\n",
    "df_obama['entities'] = extract_entities(df_obama['processed_text'])\n",
    "df_trump['entities'] = extract_entities(df_trump['processed_text'])\n",
    "df_biden['entities'] = extract_entities(df_biden['processed_text'])\n",
    "\n",
    "# Save the entities\n",
    "df_obama.to_csv('../data/entities/distilBERT_NER_results_obama.csv', index=False)\n",
    "df_trump.to_csv('../data/entities/distilBERT_NER_results_trump.csv', index=False)\n",
    "df_biden.to_csv('../data/entities/distilBERT_NER_results_biden.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666ffbe64f4101f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:02:06.371790Z",
     "start_time": "2024-10-21T13:02:06.361900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('United Nations', 'ORG'), ('Contact Group on Piracy off the Coast of Somalia', 'ORG'), ('United States', 'ORG'), ('New York Declaration', 'ORG'), ('International Ship and Port Facility Security Code', 'ORG'), ('Panama', 'ORG'), ('Bahamas', 'ORG'), ('Liberia', 'ORG'), ('Marshall Islands', 'ORG'), ('NATO', 'ORG'), ('European Union', 'ORG'), ('Somalia', 'LOC'), ('Coast of Somalia', 'LOC'), ('United States', 'LOC'), ('New York', 'LOC'), ('Horn of Africa', 'LOC'), ('piracy', 'MISC'), ('Syrian Embassy', 'ORG'), ('United States', 'ORG'), ('State Department', 'ORG'), ('Honorary Consuls', 'ORG'), ('Vienna Convention on Diplomatic Relations', 'ORG'), ('United States', 'LOC'), ('Michigan', 'LOC'), ('Texas', 'LOC'), ('Washington', 'LOC'), ('Syria', 'LOC'), ('Syrian ambassador', 'PER'), ('Clinton', 'PER'), ('Lavrov', 'PER'), ('Obama', 'PER'), ('Medvedev', 'PER'), ('United States', 'ORG'), ('G-20', 'ORG'), ('NATO', 'ORG'), ('NATO Russia Council', 'ORG'), ('Russia', 'ORG'), ('START', 'ORG'), ('Russian Federation', 'ORG'), ('Euro Atlantic', 'ORG'), ('Quartet', 'ORG'), ('Afghanistan', 'LOC'), ('Iran', 'LOC'), ('North Korea', 'LOC'), ('Georgia', 'LOC'), ('London', 'LOC'), ('Horn of Africa', 'LOC'), ('arms control', 'MISC'), ('nonproliferation', 'MISC'), ('nuclear weapons', 'MISC'), ('Judith Heumann', 'PER'), ('Mexico', 'LOC'), ('Mexico City', 'LOC'), ('United States', 'LOC'), ('United States', 'ORG'), ('international disability rights', 'MISC'), ('accessibility', 'MISC'), ('inclusive education', 'MISC'), ('Catherine A. Novelli', 'PER'), ('Marrakech', 'LOC'), ('Morocco', 'LOC'), ('United Nations Framework Convention on Climate Change', 'ORG'), ('cop-22', 'ORG'), ('Global Climate Action Day on Oceans', 'ORG'), ('Marrakech e prix', 'MISC'), ('Formula E', 'MISC'), ('Africa', 'LOC'), ('Youth leadership event', 'MISC'), ('U.S. exchange programs', 'ORG'), ('Bill Burns', 'PER'), ('World Affairs Councils of America', 'ORG'), ('Renaissance Mayflower Hotel', 'LOC'), ('Washington DC', 'LOC'), ('Asia', 'LOC'), ('Americas', 'LOC'), ('John McCain', 'PER'), ('Stephen Hadley', 'PER'), ('Richard C. Levin', 'PER'), ('Yale University', 'ORG'), ('World Affairs Councils of America', 'ORG'), ('Kelsey Compton', 'PER'), ('Reta Jo Lewis', 'PER'), ('India', 'LOC'), ('Maryland', 'LOC'), (\"Martin O'Malley\", 'PER'), ('Maryland', 'LOC'), ('United States', 'LOC'), ('India', 'LOC'), ('United States', 'ORG'), ('Southern Sudan', 'LOC'), ('Southern Sudan Referendum Commission', 'ORG'), ('ssrc', 'ORG'), ('Southern Sudan Legislative Assembly', 'ORG'), ('Antigua and Barbuda', 'LOC'), ('Bahamas', 'LOC'), ('Barbados', 'LOC'), ('Belize', 'LOC'), ('Commonwealth of Dominica', 'LOC'), ('Dominican Republic', 'LOC'), ('Grenada', 'LOC'), ('Republic of Guyana', 'LOC'), ('Republic of Haiti', 'LOC'), ('Jamaica', 'LOC'), ('St. Kitts and Nevis', 'LOC'), ('St. Lucia', 'LOC'), ('St. Vincent and the Grenadines', 'LOC'), ('Republic of Suriname', 'LOC'), ('Republic of Trinidad and Tobago', 'LOC'), ('United States', 'LOC'), ('Montego Bay', 'LOC'), ('Bridgetown', 'LOC'), ('Caribbean Basin Security Initiative', 'ORG'), ('CBSI', 'ORG'), ('United States', 'ORG'), ('Caribbean', 'LOC'), ('Haiti', 'LOC'), ('Energy and Climate Partnership of the Americas', 'ORG'), ('ecpa', 'ORG'), ('Advancing Sustainable Energy Solutions in the Caribbean initiative', 'ORG'), ('Caribbean Climate Change Adaptation Initiative', 'ORG'), ('University of the West Indies', 'ORG'), ('U.S. universities', 'ORG'), ('United States', 'ORG'), ('Caribbean', 'LOC'), ('United States', 'LOC'), ('Caribbean', 'LOC'), ('U.S. secretary of State', 'PER'), ('International Diaspora Engagement Alliance', 'ORG'), ('IDEA', 'ORG'), ('Caribbean diaspora', 'MISC'), ('caribbean idea marketplace', 'ORG'), ('Caribbean American heritage month', 'MISC')]\n"
     ]
    }
   ],
   "source": [
    "def read_manual_labels(file_path):\n",
    "    manual_labels = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Safely evaluate the line as a tuple\n",
    "            try:\n",
    "                entity = ast.literal_eval(line.strip())\n",
    "                manual_labels.append(entity)  # Append the (text, label) tuple\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing entity: {line}, Error: {e}\")\n",
    "\n",
    "    return manual_labels\n",
    "\n",
    "# Example usage (for Obama labels):\n",
    "manual_labels = read_manual_labels(\"../data/preprocessed/manual_labeling_obama.csv\")\n",
    "print(manual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6061a7d96cbe6f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:02:08.966220Z",
     "start_time": "2024-10-21T13:02:08.961045Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_ner_results(file_path, max_lines=None):\n",
    "    ner_results = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip the header line 'publish_date,entities'\n",
    "\n",
    "        for i, row in enumerate(reader):\n",
    "            if max_lines is not None and i >= max_lines:\n",
    "                break  # Stop if we reach the specified line limit\n",
    "            if len(row) < 2:\n",
    "                continue  # Skip if the row is malformed or incomplete\n",
    "\n",
    "            entities_str = row[1]  # We're only interested in the second column, which contains the entities\n",
    "            try:\n",
    "                # Safely evaluate the string containing the list of entities\n",
    "                entities = ast.literal_eval(entities_str)\n",
    "                ner_results.extend([(entity['text'], entity['label']) for entity in entities])  # Add (text, label) tuples\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing entity: {entities_str}, Error: {e}\")\n",
    "\n",
    "    return ner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "497f1b2b6bc98c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:02:09.652907Z",
     "start_time": "2024-10-21T13:02:09.651005Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation function remains the same\n",
    "def evaluate_ner(manual_labels, ner_results):\n",
    "    manual_set = set(manual_labels)\n",
    "    ner_set = set(ner_results)\n",
    "\n",
    "    true_positives = manual_set & ner_set\n",
    "    false_positives = ner_set - manual_set\n",
    "    false_negatives = manual_set - ner_set\n",
    "\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives)) if len(ner_set) > 0 else 0\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if len(manual_set) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742021db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the NER results using the distilBERT model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85955debea74fbcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T13:02:10.651219Z",
     "start_time": "2024-10-21T13:02:10.493614Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Biden NER Results:\n",
      "Precision: 0.5618\n",
      "Recall:    0.6993\n",
      "F1 Score:  0.6231\n",
      "----------------------------------------\n",
      "Evaluating Obama NER Results:\n",
      "Precision: 0.3412\n",
      "Recall:    0.5088\n",
      "F1 Score:  0.4085\n",
      "----------------------------------------\n",
      "Evaluating Trump NER Results:\n",
      "Precision: 0.5733\n",
      "Recall:    0.6615\n",
      "F1 Score:  0.6143\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Paths to the files (adjusted to the naming scheme in the screenshot)\n",
    "file_pairs = [\n",
    "    (\"../flairData/manual_labeling_biden.csv\", \"../flairData/ner_results_biden.csv\"),\n",
    "    (\"../flairData/manual_labeling_obama.csv\", \"../flairData/ner_results_obama.csv\"),\n",
    "    (\"../flairData/manual_labeling_trump.csv\", \"../flairData/ner_results_trump.csv\")\n",
    "]\n",
    "\n",
    "# Evaluate and print the results for each file pair\n",
    "for manual_file, ner_file in file_pairs:\n",
    "    president = manual_file.split('_')[-1].split('.')[0].capitalize()  # Extracting 'Biden', 'Obama', 'Trump'\n",
    "    \n",
    "    # Reading manual labels and NER results using the existing functions\n",
    "    manual_labels = read_manual_labels(manual_file)\n",
    "    ner_results = read_ner_results(ner_file, 10)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    precision, recall, f1 = evaluate_ner(manual_labels, ner_results)\n",
    "\n",
    "    # Print the evaluation results in a nice format\n",
    "    print(f\"Evaluating {president} NER Results:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
